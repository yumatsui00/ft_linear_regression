# ft_linear_regression

## 線型回帰における勾配降下法
![image](https://github.com/user-attachments/assets/17449d8b-6040-4774-a6be-40397cbfe7a5)
このような線型回帰モデルを仮定する。

勾配降下法を適用するには、まず最小化したい目的関数（損失関数）を定義する。
線型回帰では、予測値と実測値の値の差を示す平均二乗誤差（MSE）を最小化することが一般的で、目的関数は以下のように表される。

![image](https://github.com/user-attachments/assets/802ab709-8ea2-41ce-b491-fa104ea9136e)


### 勾配の計算
目的関数を最小化するために、勾配を計算する。勾配は微分を用いて計算するため、

![image](https://github.com/user-attachments/assets/2c41ae7c-358f-41c3-8871-8b9aa9b0f3b1)

となる。ここでtheta0, theta1の微分は、それぞれパラメータの更新量を表している。（調整する方向と量）

### パラメータの更新

勾配降下法では、計算した勾配を用いてパラメータを少しずつ調整し、目的関数の減少を行う。更新式は以下

![image](https://github.com/user-attachments/assets/89824b7c-e7df-4c8f-8581-1be878e9513a)

このようにしてパラメータが少しずつ調整され、誤差が少ない方向へ収束していく。

### 反復（イテレーション）

勾配降下法では、上記の手順を繰り返し、パラメータが収束するまで続けます。
イテレーションが小さすぎると、学習が不十分で、最適なパラメータに到達しない可能性があります。（未収束）
イテレーションが大きすぎると過収束、オーバーフィッティングが生じる可能性があります。（計算資源の浪費、トレーニングデータに合わせすぎる予測精度の低下、細かいノイズの学習）

これらの防止の為、早期停止の導入などを行い、適切なイテレーション数を決定する必要があります。

## かんたんなまとめ

目的関数は二次関数で、勾配降下法によってその底を求めるたい。傾きに沿って学習効率の分だけ一度に下に降下していくが、学習効率が高すぎると底を通り過ぎて傾きがプラスになってしまい、その結果発散をしてしまう。そのため、適切なパラメータの設定が必要となる。
